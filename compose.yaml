services:
  triton:
    runtime: nvidia
    build:
      context: .
      dockerfile: Dockerfile
      network: host
    shm_size: "64gb"
    ports:
      - "127.0.0.1:8900:8000"
      - "127.0.0.1:8901:8001"
      - "127.0.0.1:8902:8002"
    volumes:
      - "./:/workspace"
      - "./model_repository:/models"
    environment:
      - LC_ALL=C.UTF-8
      - LANG=C.UTF-8
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["all"]
              capabilities: [gpu]
    command: tritonserver --model-repository=/models --http-thread-count 8 --log-verbose 0 --log-error 1 --log-info 1

  tensorrt:
    image: nvcr.io/nvidia/tensorrt:23.12-py3
    stdin_open: true
    tty: true
    volumes:
      - ./sources:/models
    runtime: nvidia

  prometheus:
    image: prom/prometheus:v2.54.1
    container_name: prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prom_data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=15d"
    ports:
      - "127.0.0.1:9090:9090"
    depends_on:
      - triton

  grafana:
    image: grafana/grafana:11.2.0
    container_name: grafana
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
    ports:
      - "127.0.0.1:3000:3000"
    depends_on:
      - prometheus

volumes:
  prom_data:
  grafana_data:
